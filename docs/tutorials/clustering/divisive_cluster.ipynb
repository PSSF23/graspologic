{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.color_palette()\n",
    "sns.set(font_scale=1.75)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from graspologic.cluster import DivisiveCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is similar to the clustering algorithms introduced above like AutoGMM and K-Means but it leads to a hierarchy of clusters. Two major types of hierarchical clustering algorithms are agglomerative and divisive. The former one starts from every data point in its own cluster and gradually merges cluters in a \"bottom-up\" fashion; the latter one assumes all data points in the same cluster initially and gradually divides it in a \"top-down\" fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DivisiveCluster algorithm implements hierarchical clustering in a “divisive” approach based on a chosen clustering algorithm such as AutoGMM. It retrieves predictions on the full dataset from the chosen clustering algorithm, say AutoGMM, and passes each subset of data corresponding to a predicted cluster onto AutoGMM again while specifying min_components=1. If the best model computed by AutoGMM for any predicted cluster leads to more than one subcluster, each of the subclusters will be clustered recursively as described above; otherwise, that subcluster becomes a leaf cluster. The algorithm terminates when all branches of recursive clustering have led to a set of leaf clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following synthetic hierarchical data made up of two levels of four Gaussian distributions in 1D. Each Gaussian distribution has a standard deviation of 0.1. The 4 means are symmetric about 0; the smallest 2 and largest 2 means are symmetric about -2.5 and 2.5, respectively. Hence, this dataset can be classified into 4 clusters of 1 Gaussian component or 2 clusters of Gaussian mixtures of 4 components. Those are the two clustering hierarchies of increasing granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synethetic data\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100  # number of data points\n",
    "d = 3  # number of dimensions\n",
    "\n",
    "# Let Xij denote the ith Gaussian mixture component in the jth cluster at the lowest hierarchy\n",
    "X11 = np.random.normal(-3, 0.5, size=(n, d))\n",
    "X21 = np.random.normal(-2, 0.5, size=(n, d))\n",
    "X12 = np.random.normal(2, 0.5, size=(n, d))\n",
    "X22 = np.random.normal(3, 0.5, size=(n, d))\n",
    "X = np.vstack((X11, X21, X12, X22))\n",
    "\n",
    "# true label at either level\n",
    "y_lvl1 = np.repeat([0, 1], 2 * n)\n",
    "y_lvl2 = np.repeat([0, 1, 2, 3], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# fit model and predict on data\n",
    "dc = DivisiveCluster(max_components=2, cluster_method=\"gmm\")\n",
    "\n",
    "# enable \"fcluster\" to return a set of flat clusterings\n",
    "pred = dc.fit_predict(X, fcluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize clustering dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical dendrogram or tree is a way to represent clusters level by level (from the root to the leaves). We will plot out hierarchical clustering results such that each cluster at each level is denoted by a unique color and each node in the dendrogram is colored by its predicted cluster at each level. Since the root level contains only 1 cluster which is trivial, we will show levels below the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the labels so that the clusters in each array recieve increasingly-indexed labels\n",
    "def relabel(pred):\n",
    "    for i in range(pred.shape[1]):\n",
    "        temp = pred[:,i].copy()\n",
    "        _, index = np.unique(temp, return_index=True)\n",
    "        # return unique labels in the order of their appearance\n",
    "        uni_labels = temp[np.sort(index)]\n",
    "        for label, ul in enumerate(uni_labels):\n",
    "            inds = temp == ul\n",
    "            temp[inds] = -label-1\n",
    "        pred[:,i] = -(temp+1)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot hierarchical clustering assignments\n",
    "\n",
    "def plot(labels, n_level, title):\n",
    "    fig,axs = plt.subplots(n_level,1, figsize=(20, n_level+1.5), sharex=True, sharey=True)\n",
    "    for i in range(n_level): \n",
    "        ax = axs[i]\n",
    "        sns.heatmap(labels[:, i].reshape((1,-1))+1, cbar=False, xticklabels=100, yticklabels=\"\", center=0, cmap='RdBu_r', ax=ax)\n",
    "        if i < n_level-1:\n",
    "            ax.set(xticklabels='')\n",
    "        else:\n",
    "            ax.set_xlabel(\"Node Index\")\n",
    "        ax.set_ylabel(i+1)\n",
    "    fig.text(0.1, 0.5, \"Level\", rotation=90, va=\"center\", ha=\"center\")\n",
    "    fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true clustering dendrogram\n",
    "\n",
    "y_true = np.vstack((y_lvl1, y_lvl2)).T\n",
    "n_level = 2\n",
    "\n",
    "plot(y_true, n_level, \"True Clustering Dendrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated clustering dendrogram\n",
    "\n",
    "pred = relabel(pred)\n",
    "n_level = pred.shape[1]\n",
    "\n",
    "plot(pred, n_level, \"Estimated Clustering Dendrogram\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}